{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "# from torchvision import transforms\n",
    "# from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from alive_progress import alive_bar\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing pickle files to extract caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/Users/samankhan/Documents/capstone_project/Github_project/generate_ai_room_designs/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "import pickle\n",
    "text_path = os.path.join(main_path,'data/extracted_data/ikea-master/text_data/')\n",
    "save_path = os.path.join(main_path,'data/pickle_processed/')\n",
    "\n",
    "pickle_files = os.listdir(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting products_dict.p pickle file\n",
      "Annotations file created at: /Users/samankhan/Documents/capstone_project/Github_project/generate_ai_room_designs/data/pickle_processed/\n"
     ]
    }
   ],
   "source": [
    "for pickle_file in [pickle_files[0]]:\n",
    "    print('extracting {} pickle file'.format(pickle_file))\n",
    "    with open(os.path.join(text_path,pickle_file), 'rb') as file:\n",
    "        deserialized_data = pickle.load(file)\n",
    "\n",
    "annotations = []\n",
    "for item_id, details in deserialized_data.items():\n",
    "    annotations.append({\n",
    "        \"file_name\" : item_id+'.jpg',\n",
    "        \"desc\" : \". \".join([details['type'], details['color'],details['size'],details['desc']])\n",
    "    })\n",
    "\n",
    "with open(os.path.join(save_path,pickle_file.replace('.p','.json')), 'w') as annotations_file:\n",
    "    json.dump(annotations, annotations_file, indent=4)\n",
    "\n",
    "print(f\"Annotations file created at: {save_path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting products_dict.p pickle file\n",
      "extracting img_to_desc.p pickle file\n",
      "Annotations file created at: /Users/samankhan/Documents/capstone_project/Github_project/generate_ai_room_designs/data/pickle_processed/\n"
     ]
    }
   ],
   "source": [
    "for pickle_file in pickle_files[:2]:\n",
    "    print('extracting {} pickle file'.format(pickle_file))\n",
    "    with open(os.path.join(text_path,pickle_file), 'rb') as file:\n",
    "        deserialized_data = pickle.load(file)\n",
    "\n",
    "annotations  = []                                            \n",
    "for item_id, details  in deserialized_data.items():\n",
    "    file_name = item_id.split('/')[-1]\n",
    "    annotations.append({\n",
    "        \"file_name\" : file_name,\n",
    "        \"desc\" : details['desc']\n",
    "    })\n",
    "    \n",
    "with open(os.path.join(save_path,pickle_file.replace('.p','.json')), 'w') as annotations_file:\n",
    "    json.dump(annotations, annotations_file, indent=4)\n",
    "\n",
    "print(f\"Annotations file created at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting products_dict.p pickle file\n",
      "extracting img_to_desc.p pickle file\n",
      "extracting categories_images_dict.p pickle file\n",
      "Annotations file created at: /Users/samankhan/Documents/capstone_project/Github_project/generate_ai_room_designs/data/pickle_processed/\n"
     ]
    }
   ],
   "source": [
    "for pickle_file in pickle_files[:3]:\n",
    "    print('extracting {} pickle file'.format(pickle_file))\n",
    "    with open(os.path.join(text_path,pickle_file), 'rb') as file:\n",
    "        deserialized_data = pickle.load(file)\n",
    "\n",
    "annotations  = []                                            \n",
    "for item_id, details  in deserialized_data.items():\n",
    "    # file_name = item_id.split('/')[-1]\n",
    "    annotations.append({\n",
    "        \"file_name\" : item_id+'.jpg',\n",
    "        \"desc\" : details\n",
    "    })\n",
    "with open(os.path.join(save_path,pickle_file.replace('.p','.json')), 'w') as annotations_file:\n",
    "    json.dump(annotations, annotations_file, indent=4)\n",
    "\n",
    "print(f\"Annotations file created at: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting products_dict.p pickle file\n",
      "extracting img_to_desc.p pickle file\n",
      "extracting categories_images_dict.p pickle file\n",
      "extracting item_to_room.p pickle file\n",
      "extracting room_to_items.p pickle file\n",
      "Annotations file created at: /Users/samankhan/Documents/capstone_project/Github_project/generate_ai_room_designs/data/pickle_processed/\n"
     ]
    }
   ],
   "source": [
    "for pickle_file in pickle_files[:5]:\n",
    "    print('extracting {} pickle file'.format(pickle_file))\n",
    "    with open(os.path.join(text_path,pickle_file), 'rb') as file:\n",
    "        deserialized_data = pickle.load(file)\n",
    "\n",
    "annotations  = []                                            \n",
    "for item_id, details  in deserialized_data.items():\n",
    "    file_name = item_id.split('/')[-1]\n",
    "    annotations.append({\n",
    "        \"file_name\" : file_name,\n",
    "        \"desc\" : ','.join(details)\n",
    "    })\n",
    "\n",
    "with open(os.path.join(save_path,pickle_file.replace('.p','.json')), 'w') as annotations_file:\n",
    "    json.dump(annotations, annotations_file, indent=4)\n",
    "\n",
    "print(f\"Annotations file created at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting products_dict.p pickle file\n",
      "extracting img_to_desc.p pickle file\n",
      "extracting categories_images_dict.p pickle file\n",
      "extracting item_to_room.p pickle file\n",
      "extracting room_to_items.p pickle file\n",
      "extracting categories_dict.p pickle file\n",
      "Annotations file created at: /Users/samankhan/Documents/capstone_project/Github_project/generate_ai_room_designs/data/pickle_processed/\n"
     ]
    }
   ],
   "source": [
    "for pickle_file in pickle_files[:6]:\n",
    "    print('extracting {} pickle file'.format(pickle_file))\n",
    "    with open(os.path.join(text_path,pickle_file), 'rb') as file:\n",
    "        deserialized_data = pickle.load(file)\n",
    "\n",
    "annotations  = []                                            \n",
    "for item_id, details  in deserialized_data.items():\n",
    "    file_name = item_id.split('/')[-1]\n",
    "    annotations.append({\n",
    "        \"file_name\" : file_name,\n",
    "        \"desc\" : details\n",
    "    })\n",
    "\n",
    "with open(os.path.join(save_path,pickle_file.replace('.p','.json')), 'w') as annotations_file:\n",
    "    json.dump(annotations, annotations_file, indent=4)\n",
    "\n",
    "print(f\"Annotations file created at: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [13:26<00:00, 403.43s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.17s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m raw_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(requests\u001b[39m.\u001b[39mget(img_url, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mraw)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhow many dogs are in the picture?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m inputs \u001b[39m=\u001b[39m processor(raw_image, question, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(processor\u001b[39m.\u001b[39mdecode(out[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:231\u001b[0m, in \u001b[0;36mBatchFeature.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    228\u001b[0m     \u001b[39m# check if v is a floating point\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_floating_point(v):\n\u001b[1;32m    230\u001b[0m         \u001b[39m# cast and send to device\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m         new_data[k] \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    232\u001b[0m     \u001b[39melif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         new_data[k] \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "question = \"how many dogs are in the picture?\"\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (1.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/samankhan/Library/Python/3.10/lib/python/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (0.23.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
